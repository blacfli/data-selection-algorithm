{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(10, str(Path(os.getcwd()).resolve().parents[0]) + '/')\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tomotopy as tp\n",
    "from utils.data_selection import DocumentSelection\n",
    "from utils.preprocesslib import Preprocess\n",
    "from utils.metrics import calculate_perplexity, calculate_coherence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data for Reuters or Wikipedia dataset\n",
    "FOLDER_PATH = sys.path[-1] + 'dataset/reuters.csv'\n",
    "# FOLDER_PATH = sys.path[-1] + 'dataset/wiki.csv'\n",
    "\n",
    "reference_documents = pd.read_csv(FOLDER_PATH)['texts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentSelector = DocumentSelection(reference_documents, name='Reuters')\n",
    "DocumentSelector.document_selector_hyper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentSelector.select_hyper_document(threshold=\"PUT_THRESHOLD_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_documents_hyper = pd.DataFrame(data={'texts': DocumentSelector.selected_doc_hyper})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess()\n",
    "(reference_corpus, our_model_selected_corpus) = preprocess.make_training_corpora(reference_documents, selected_documents_hyper, lemma_model='efficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = \"CHOOSE_MAXIMUM_NUMBER_OF_TOPICS\"\n",
    "def topic_model_compare(topic_reference_corpus, model_selected_corpus):\n",
    "    pbar = tqdm(total = n * 1, file = sys.stdout, ascii = ' >=')\n",
    "    model_results = {'Topics'    : [],\n",
    "                     'Perplexity': [],\n",
    "                     'c_v'       : []\n",
    "                    }\n",
    "    for i in range(1, n+1):\n",
    "        mdl = tp.LDAModel(k=i, seed = 100, corpus = model_selected_corpus)\n",
    "        mdl.train(1000)\n",
    "        perplexity_score = calculate_perplexity(model=mdl, corpus=model_selected_corpus)\n",
    "        model_results['Topics'].append(i)\n",
    "        model_results['Perplexity'].append(perplexity_score)\n",
    "        \n",
    "        for preset in ['c_v']:\n",
    "            average_coherence_score = calculate_coherence(model=mdl, topic_reference_corpus=topic_reference_corpus, preset=preset)\n",
    "            model_results[preset].append(average_coherence_score)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return model_results\n",
    "\n",
    "def topic_model(topic_reference_corpus):\n",
    "    pbar = tqdm(total = n * 1, file = sys.stdout, ascii = ' >=')\n",
    "    model_results = {'Topics'    : [],\n",
    "                     'Perplexity': [],\n",
    "                     'c_v'       : []\n",
    "                    }\n",
    "    for i in range(1, n+1):\n",
    "#         mdl = tp.LDAModel(min_df=3, rm_top=10, k=i, seed = 100, corpus = original)\n",
    "        mdl = tp.LDAModel(k=i, seed = 100, corpus = topic_reference_corpus)\n",
    "        mdl.train(1000)\n",
    "        \n",
    "        perplexity_score = calculate_perplexity(model=mdl, corpus=topic_reference_corpus)\n",
    "        model_results['Topics'].append(i)\n",
    "        model_results['Perplexity'].append(perplexity_score)\n",
    "        \n",
    "        for preset in ['c_v']:\n",
    "            average_coherence_score = tp.coherence.Coherence(mdl, coherence=preset, top_n = 10).get_score()\n",
    "            model_results[preset].append(average_coherence_score)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_results_hyper = topic_model_compare(reference_corpus, our_model_selected_corpus)\n",
    "model_results_reference = topic_model(reference_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. If you want to save the results (uncomment the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_results/PUT_FILENAME_HERE.pkl', 'wb') as handle:\n",
    "#     pickle.dump(model_results_reference, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_results/PUT_FILENAME_HERE.pkl', 'wb') as handle:\n",
    "#     pickle.dump(model_results_hyper, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
